{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ced68d4",
   "metadata": {},
   "source": [
    "### Get steered model generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98123e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emste\\Documents\\cloned_Gits\\model_steering_multilingual\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1873f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we could also custom steer only at one token position like this, might be good for performance... \n",
    "COEFFICIENT = 1.0\n",
    "STEER_LAYER = 12 ## change this to the layer you want to steer\n",
    "STEER_VECTOR = torch.load(\"C:\\\\Users\\\\emste\\\\Documents\\\\cloned_Gits\\\\model_steering_multilingual\\\\external\\\\refusal_direction\\\\pipeline\\\\runs\\\\meta-llama-3-8b-instruct\\\\direction.pt\", map_location=torch.device('cpu')) ## take cpu out here\n",
    "BATCH_SIZE = 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5f19c",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'Qwen/Qwen-1_8B-chat'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float16,\n",
    "    default_padding_side='left',\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "model.tokenizer.pad_token = '<|extra_0|>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac6dba",
   "metadata": {},
   "source": [
    "### Tokenize instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b82082",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    prompts = [QWEN_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True,truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_qwen_chat, tokenizer=model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e67cc7c",
   "metadata": {},
   "source": [
    "### Hook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc2cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_addition_pre_hook(\n",
    "        value: Float[torch.Tensor, \"batch seq_len d_model\"], ## not sure what shape this should have? [batch_size, seq_len, dim] or [batch_size, seq_len, d_model] I think...\n",
    "        hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "    \"\"\"Pre hook for adding a steering vector with coefficient.\"\"\"\n",
    "\n",
    "    print(f\"Shape of input values: {value.shape}\")\n",
    "    print(f\"Hook input: {hook}\")\n",
    "    print(f\"Shape of steer vector: {STEER_VECTOR.shape}\")\n",
    "\n",
    "    STEER_VECTOR.to(value.device)  \n",
    "    steered_value = value + COEFFICIENT * STEER_VECTOR # what do we do with the batch dimension? ## def sanity check that this works over all the samples in a batch...\n",
    "\n",
    "    return steered_value\n",
    "\n",
    "\n",
    "fwd_hooks=[(\n",
    "        utils.get_act_name(\"resid_pre\", STEER_LAYER),  ## not 100% sure which hook type to use here.\n",
    "        layer_addition_pre_hook\n",
    "        )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d3277",
   "metadata": {},
   "source": [
    "### Generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with hooks\n",
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],## should be able to handle multiple hooks\n",
    ") -> List[str]:\n",
    "\n",
    "    # \"maske\" fÃ¼r die input und output tokens\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device) # torch.long = torch.int64\n",
    "\n",
    "    # fill mask with input tokens\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    # der with block scoped automaically wo die hooks aktiv sind judn removed nachher automatically\n",
    "    # go always one token position further for every thing in the batch so the model doesnt see the zeroes only everything before\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i]) # model(tokens) is same as model.forward but with better hook use\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0), since we do greedy sampling, we dont need a softmax here\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True) # decode only generated tokens\n",
    "\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 4,\n",
    ") -> List[str]:\n",
    "\n",
    "    generations = []\n",
    "    \n",
    "  \n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "        generations.extend(generation)\n",
    "\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3102e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
