INFO:lm_eval.__main__:Selected Tasks: ['multijail']
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'meta-llama/meta-llama-3-8b-instruct'}
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
Fetching 4 files: 100%|██████████| 4/4 [01:00<00:00, 15.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 13.15s/it]
WARNING:lm_eval.api.task:multijail: No `until` specified in `generation_kwargs`! Defaulting to the fewshot_delimiter='\n\n'
INFO:lm_eval.evaluator:multijail: Using gen_kwargs: {'max_gen_toks': 100, 'temperature': 0.7, 'do_sample': True, 'until': ['\n\n']}
INFO:lm_eval.evaluator:Processing multijail in output-only mode. Metrics will not be calculated!
WARNING:lm_eval.evaluator:Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
INFO:lm_eval.api.task:Building contexts for multijail on rank 0...
100%|██████████| 3150/3150 [00:00<00:00, 3183.69it/s]
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   2%|▏         | 57/3150 [01:28<31:13,  1.65it/s]  
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 8
